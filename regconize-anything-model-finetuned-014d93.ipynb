{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1874598,"sourceType":"datasetVersion","datasetId":1115942},{"sourceId":283615886,"sourceType":"kernelVersion"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng v√† t·∫£i Weights","metadata":{}},{"cell_type":"code","source":"# 1. C√†i ƒë·∫∑t th∆∞ vi·ªán\n!pip install timm==\"0.4.12\" transformers==\"4.30.2\" fairscale==\"0.4.4\" pycocoevalcap scikit-learn\n\n# 2. Clone Repo\nimport os\nif not os.path.exists('recognize-anything'):\n    !git clone https://github.com/xinyu1205/recognize-anything.git\n%cd recognize-anything\n\n# 3. T·∫£i Weights (C√ì KI·ªÇM TRA DUNG L∆Ø·ª¢NG)\ndef smart_download(url, filename, min_size_bytes):\n    should_download = True\n    if os.path.exists(filename):\n        # Ki·ªÉm tra k√≠ch th∆∞·ªõc file\n        file_size = os.path.getsize(filename)\n        if file_size < min_size_bytes:\n            print(f\"C·∫¢NH B√ÅO: File '{filename}' b·ªã l·ªói (Size: {file_size/1024/1024:.1f} MB). ƒêang x√≥a ƒë·ªÉ t·∫£i l·∫°i...\")\n            os.remove(filename)\n        else:\n            print(f\"File '{filename}' ƒë√£ c√≥ s·∫µn v√† h·ª£p l·ªá ({file_size/1024/1024:.1f} MB).\")\n            should_download = False\n            \n    if should_download:\n        print(f\"ƒêang t·∫£i {filename}...\")\n        !wget {url} -O {filename}\n        \n        # Ki·ªÉm tra l·∫°i l·∫ßn n·ªØa sau khi t·∫£i\n        if os.path.exists(filename) and os.path.getsize(filename) > min_size_bytes:\n            print(\"üéâ T·∫£i th√†nh c√¥ng!\")\n        else:\n            print(\"‚ùå T·∫£i th·∫•t b·∫°i! Vui l√≤ng ki·ªÉm tra m·∫°ng.\")\n\n# RAM Large\nsmart_download(\n    \"https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/ram_swin_large_14m.pth\",\n    \"ram_swin_large_14m.pth\",\n    1_500_000_000 \n)\n\n# Tag2Text\nsmart_download(\n    \"https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/tag2text_swin_14m.pth\",\n    \"tag2text_swin_14m.pth\",\n    800_000_000\n)\n\nprint(\"Setup Ho√†n T·∫•t!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T16:56:10.010776Z","iopub.execute_input":"2025-12-03T16:56:10.011136Z","iopub.status.idle":"2025-12-03T17:02:44.195047Z","shell.execute_reply.started":"2025-12-03T16:56:10.011107Z","shell.execute_reply":"2025-12-03T17:02:44.192501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Customize l·∫°i bert_config v√† tag2text","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\n# fix vocab bert config lai5 chuan63 30524\nbert_config_path = 'ram/models/bert_config.json'\nconfig_content = {\n  \"architectures\": [\"BertModel\"], \"attention_probs_dropout_prob\": 0.1, \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1, \"hidden_size\": 768, \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072, \"layer_norm_eps\": 1e-12, \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\", \"num_attention_heads\": 12, \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0, \"type_vocab_size\": 2, \"vocab_size\": 30524, \"fusion_layer\": 0, \"encoder_width\": 768\n}\nos.makedirs(os.path.dirname(bert_config_path), exist_ok=True)\nwith open(bert_config_path, 'w') as f:\n    json.dump(config_content, f)\nprint(\"ƒê√£ v√° file bert_config.json\")\n\n# fix import c·ªßa tag2text cho ph√π h·ª£p v·ªõi kaggle\ntag2text_code = \"\"\"\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom .swin_transformer import SwinTransformer\nfrom .bert import BertConfig, BertLMHeadModel, BertModel\nimport numpy as np\nimport os\nfrom transformers import BertTokenizer\n\ndef build_tokenizer(text_encoder_type):\n    tokenizer = BertTokenizer.from_pretrained(text_encoder_type)\n    tokenizer.add_special_tokens({'bos_token': '[DEC]', 'additional_special_tokens': ['[ENC]']})\n    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]\n    return tokenizer\n\ndef create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer):\n    if vit == 'swin_b':\n        return SwinTransformer(img_size=image_size, patch_size=4, in_chans=3, embed_dim=128, depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32], window_size=12), 1024\n    elif vit == 'swin_l' or vit == 'large':\n        return SwinTransformer(img_size=image_size, patch_size=4, in_chans=3, embed_dim=192, depths=[2, 2, 18, 2], num_heads=[6, 12, 24, 48], window_size=12), 1536\n    return None, 0\n\ndef load_checkpoint_swin(model, url_or_filename):\n    checkpoint = torch.load(url_or_filename, map_location='cpu')\n    state_dict = checkpoint.get('model', checkpoint)\n    full_dict = model.state_dict()\n    for k in list(state_dict.keys()):\n        if 'relative_position_bias_table' in k:\n            if state_dict[k].size() != full_dict[k].size():\n                state_dict[k] = F.interpolate(state_dict[k].permute(1, 0).unsqueeze(0), size=full_dict[k].shape[0], mode='linear', align_corners=False).squeeze(0).permute(1, 0)\n    model.load_state_dict(state_dict, strict=False)\n    return model, \"Loaded\"\n\nclass Tag2Text(nn.Module):\n    def __init__(self, image_size=384, text_encoder_type='bert-base-uncased', vit='base', tag_list='../data/tag_list.txt', **kwargs):\n        super().__init__()\n        self.tokenizer = build_tokenizer(text_encoder_type)\n        self.tag_list = self.load_tag_list(tag_list)\n        self.num_class = len(self.tag_list)\n        self.visual_encoder, vision_width = create_vit(vit, image_size, False, 0)\n        med_config = BertConfig.from_json_file(f'{os.path.dirname(os.path.abspath(__file__))}/bert_config.json')\n        med_config.encoder_width = vision_width\n        self.text_decoder = BertLMHeadModel(config=med_config)\n        self.tagging_head = nn.Linear(vision_width, self.num_class)\n        self.label_embed = nn.Embedding(self.num_class, 512)\n        \n    def load_tag_list(self, tag_list_file):\n        with open(tag_list_file, 'r') as f: return np.array(f.read().splitlines())\n\n    def forward(self, image, caption=None):\n        image_embeds = self.visual_encoder(image)\n        return self.tagging_head(image_embeds.mean(dim=1))\n\ndef tag2text(pretrained='', **kwargs):\n    model = Tag2Text(**kwargs)\n    if pretrained: load_checkpoint_swin(model, pretrained)\n    return model\n\"\"\"\nwith open('ram/models/tag2text.py', 'w') as f:\n    f.write(tag2text_code)\nprint(\"ƒê√£ v√° file ram/models/tag2text.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:04:55.844159Z","iopub.execute_input":"2025-12-03T17:04:55.844930Z","iopub.status.idle":"2025-12-03T17:04:55.862112Z","shell.execute_reply.started":"2025-12-03T17:04:55.844862Z","shell.execute_reply":"2025-12-03T17:04:55.860826Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# X·ª¨ L√ù D·ªÆ LI·ªÜU (CHIA TRAIN/TEST, T·∫†O CAPTION v√† TAG LIST)","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport glob\nimport random\nfrom sklearn.model_selection import train_test_split\n\n\n# PH·∫¶N 1: X·ª¨ L√ù D·ªÆ LI·ªÜU & T·∫†O FILE JSON\n\n# Config\nBASE_DIR = '/kaggle/input' \nTRAIN_JSON = 'garbage_train.json'\nTEST_JSON = 'garbage_test.json'\nT2T_JSON = 'tag2text_train.json'\nTAG_FILE = 'garbage_tags.txt'\nCLASSES = ['glass', 'paper', 'cardboard', 'plastic', 'metal', 'battery', 'trash']\n\nprint(\"üîÑ ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...\")\n\n# 1. T·∫°o file tag list\nwith open(TAG_FILE, 'w') as f:\n    for c in CLASSES: f.write(f\"{c}\\n\")\n\n# 2. Qu√©t ·∫£nh t·ª´ th∆∞ m·ª•c Input\nlabel_map = {\n    'glass': ['brown-glass', 'green-glass', 'white-glass', 'glass'],\n    'paper': ['paper'], 'cardboard': ['cardboard'], 'plastic': ['plastic'],\n    'metal': ['metal'], 'battery': ['battery'],\n    'trash': ['trash', 'biological', 'shoes', 'clothes']\n}\nfolder_to_label = {src: target for target, srcs in label_map.items() for src in srcs}\n\nall_data = []\nfor root, dirs, files in os.walk(BASE_DIR):\n    for d in dirs:\n        if d.lower() in folder_to_label:\n            lbl = folder_to_label[d.lower()]\n            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG']:\n                for p in glob.glob(os.path.join(root, d, ext)):\n                    all_data.append({\"image_path\": p, \"labels\": [lbl]})\n\nif len(all_data) > 0:\n    # 3. Chia Train/Test (80/20)\n    random.shuffle(all_data)\n    train, test = train_test_split(all_data, test_size=0.2, random_state=42)\n    \n    with open(TRAIN_JSON, 'w') as f: json.dump(train, f, indent=4)\n    with open(TEST_JSON, 'w') as f: json.dump(test, f, indent=4)\n    \n    # 4. T·∫°o d·ªØ li·ªáu cho Tag2Text (Sinh caption gi·∫£)\n    templates = [\"A photo of {}.\", \"This is {}.\", \"Image of {}.\"]\n    t2t_data = [{\"image_path\": i['image_path'], \"tag\": i['labels'], \"caption\": random.choice(templates).format(i['labels'][0])} for i in train]\n    with open(T2T_JSON, 'w') as f: json.dump(t2t_data, f, indent=4)\n    \n    print(f\"‚úÖ X·ª≠ l√Ω d·ªØ li·ªáu xong: Train ({len(train)}) | Test ({len(test)})\")\nelse:\n    print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y ·∫£nh! H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n Input.\")\n\n\n# PH·∫¶N 2: T·∫†O FILE CODE 'custom_datasets.py'\n\ndataset_code_content = \"\"\"\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport json\nfrom transformers import BertTokenizer\n\nCLASSES = ['glass', 'paper', 'cardboard', 'plastic', 'metal', 'battery', 'trash']\n\nclass GarbageDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        with open(json_file, 'r') as f: self.data = json.load(f)\n        self.transform = transform\n        self.classes = CLASSES\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n\n    def __len__(self): return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        try: image = Image.open(item['image_path']).convert('RGB')\n        except: image = Image.new('RGB', (384, 384))\n        \n        if self.transform: image = self.transform(image)\n        \n        label_tensor = torch.zeros(len(self.classes))\n        # H·ªó tr·ª£ c·∫£ key 'labels' (RAM) v√† 'tag' (Tag2Text)\n        labels = item.get('labels', item.get('tag', []))\n        \n        for name in labels:\n            if name in self.class_to_idx:\n                label_tensor[self.class_to_idx[name]] = 1.0\n        return image, label_tensor\n\nclass Tag2TextDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        with open(json_file, 'r') as f: self.data = json.load(f)\n        self.transform = transform\n        self.classes = CLASSES\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def __len__(self): return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        try: image = Image.open(item['image_path']).convert('RGB')\n        except: image = Image.new('RGB', (384, 384))\n        \n        if self.transform: image = self.transform(image)\n        \n        tag_tensor = torch.zeros(len(self.classes))\n        tags = item.get('tag', item.get('labels', []))\n        \n        for t in tags:\n            if t in self.class_to_idx: tag_tensor[self.class_to_idx[t]] = 1.0\n            \n        caption = item.get('caption', \"\")\n        encoding = self.tokenizer(caption, padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\")\n        return image, tag_tensor, encoding.input_ids.squeeze(), encoding.attention_mask.squeeze()\n\"\"\"\n\n# Ghi n·ªôi dung v√†o file\nwith open('custom_datasets.py', 'w') as f:\n    f.write(dataset_code_content)\n\nprint(\"ƒê√£ t·∫°o xong file code: custom_datasets.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:04:59.522206Z","iopub.execute_input":"2025-12-03T17:04:59.522485Z","iopub.status.idle":"2025-12-03T17:05:27.113818Z","shell.execute_reply.started":"2025-12-03T17:04:59.522468Z","shell.execute_reply":"2025-12-03T17:05:27.113027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ƒê·ªäNH NGHƒ®A DATASET CLASSES","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom transformers import BertTokenizer\nimport json\n\nclass GarbageDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        with open(json_file) as f: self.data = json.load(f)\n        self.transform = transform\n        self.cls_map = {c: i for i, c in enumerate(CLASSES)}\n    def __len__(self): return len(self.data)\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        try: img = Image.open(item['image_path']).convert('RGB')\n        except: img = Image.new('RGB', (384, 384))\n        if self.transform: img = self.transform(img)\n        lbl = torch.zeros(len(CLASSES)); lbl[self.cls_map[item['labels'][0]]] = 1.0\n        return img, lbl\n\nclass Tag2TextDataset(Dataset):\n    def __init__(self, json_file, transform=None):\n        with open(json_file) as f: self.data = json.load(f)\n        self.transform = transform\n        self.cls_map = {c: i for i, c in enumerate(CLASSES)}\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    def __len__(self): return len(self.data)\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        try: img = Image.open(item['image_path']).convert('RGB')\n        except: img = Image.new('RGB', (384, 384))\n        if self.transform: img = self.transform(img)\n        tag = torch.zeros(len(CLASSES)); tag[self.cls_map[item['tag'][0]]] = 1.0\n        enc = self.tokenizer(item['caption'], padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\")\n        return img, tag, enc.input_ids.squeeze(), enc.attention_mask.squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:05:31.479625Z","iopub.execute_input":"2025-12-03T17:05:31.480551Z","iopub.status.idle":"2025-12-03T17:05:40.328523Z","shell.execute_reply.started":"2025-12-03T17:05:31.480527Z","shell.execute_reply":"2025-12-03T17:05:40.327302Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING RAM (SWIM LARGE)","metadata":{}},{"cell_type":"code","source":"%%writefile train_ram.py\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom ram.models import ram\nfrom custom_datasets import GarbageDataset\nimport numpy as np\nfrom torch.cuda.amp import autocast, GradScaler\nimport gc\n\n# --- C·∫§U H√åNH ---\nPRETRAINED = 'ram_swin_large_14m.pth'\nTRAIN_FILE = 'garbage_train.json'\n# Batch size nh·ªè (4) gi√∫p CutMix ho·∫°t ƒë·ªông hi·ªáu qu·∫£ tr√™n P100 m√† kh√¥ng tr√†n RAM\nBATCH_SIZE = 4\nLR = 1e-5\nEPOCHS = 1\n\n# --- H√ÄM T·∫†O H·ªòP C·∫ÆT (BOUNDING BOX) ---\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    # T√≠nh k√≠ch th∆∞·ªõc v√πng c·∫Øt d·ª±a tr√™n t·ª∑ l·ªá lambda\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    # Ch·ªçn t√¢m ng·∫´u nhi√™n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    # T√≠nh to·∫° ƒë·ªô 4 g√≥c\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef main():\n    gc.collect(); torch.cuda.empty_cache()\n    device = torch.device('cuda')\n    \n    # Transform chu·∫©n\n    transform = transforms.Compose([\n        transforms.Resize((384, 384)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    ds = GarbageDataset(TRAIN_FILE, transform)\n    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n\n    print(\"üöÄ Training RAM v·ªõi chi·∫øn thu·∫≠t CutMix (Mosaic)...\")\n    \n    model = ram(pretrained=PRETRAINED, image_size=384, vit='swin_l')\n    model.tagging_head = nn.Linear(1536, 7)\n    model = model.to(device)\n    \n    # M·ªü kh√≥a to√†n b·ªô ƒë·ªÉ h·ªçc features c·ª•c b·ªô\n    for p in model.parameters(): p.requires_grad = True\n    \n    optim = torch.optim.AdamW(model.parameters(), lr=LR)\n    crit = nn.BCEWithLogitsLoss()\n    scaler = GradScaler()\n\n    for epoch in range(EPOCHS):\n        model.train()\n        total_loss = 0\n        for i, (img, tgt) in enumerate(dl):\n            img, tgt = img.to(device), tgt.to(device)\n            optim.zero_grad()\n            \n            with autocast():\n                # --- THU·∫¨T TO√ÅN CUTMIX ---\n                # √Åp d·ª•ng 50% s·ªë l·∫ßn (ƒë·ªÉ model v·∫´n h·ªçc ƒë∆∞·ª£c ·∫£nh g·ªëc)\n                if np.random.rand() < 0.5:\n                    # 1. T·∫°o lambda (t·ª∑ l·ªá di·ªán t√≠ch)\n                    lam = np.random.beta(1.0, 1.0)\n                    \n                    # 2. Ho√°n ƒë·ªïi v·ªã tr√≠ ·∫£nh trong batch\n                    rand_index = torch.randperm(img.size()[0]).cuda()\n                    target_a = tgt\n                    target_b = tgt[rand_index]\n                    \n                    # 3. T·∫°o v√πng c·∫Øt\n                    bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n                    \n                    # 4. D√°n ·∫£nh B ƒë√® l√™n v√πng c·∫Øt c·ªßa ·∫£nh A\n                    img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n                    \n                    # 5. T√≠nh l·∫°i t·ª∑ l·ªá lambda ch√≠nh x√°c theo pixel\n                    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n                    \n                    # 6. Forward & T√≠nh Loss h·ªón h·ª£p\n                    embeds = model.visual_encoder(img)\n                    if len(embeds.shape)==3: embeds = embeds.mean(dim=1)\n                    output = model.tagging_head(embeds)\n                    loss = crit(output, target_a) * lam + crit(output, target_b) * (1. - lam)\n                    \n                else:\n                    # Train ·∫£nh th∆∞·ªùng\n                    embeds = model.visual_encoder(img)\n                    if len(embeds.shape)==3: embeds = embeds.mean(dim=1)\n                    loss = crit(model.tagging_head(embeds), tgt)\n            \n            scaler.scale(loss).backward(); scaler.step(optim); scaler.update()\n            total_loss += loss.item()\n            \n            if i%100==0: print(f\"Ep {epoch+1} Step {i} Loss: {loss.item():.4f}\")\n        \n        print(f\"=== Epoch {epoch+1} Avg Loss: {total_loss/len(dl):.4f} ===\")\n        torch.save(model.state_dict(), \"ram_best.pth\")\n\nif __name__ == '__main__': main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:05:45.402898Z","iopub.execute_input":"2025-12-03T17:05:45.403320Z","iopub.status.idle":"2025-12-03T17:05:45.412556Z","shell.execute_reply.started":"2025-12-03T17:05:45.403303Z","shell.execute_reply":"2025-12-03T17:05:45.411143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING TAG2TEXT (SWIM BASE)","metadata":{}},{"cell_type":"code","source":"%%writefile train_tag2text.py\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch.cuda.amp import autocast, GradScaler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertConfig, BertLMHeadModel\nimport os\nimport numpy as np\n# Import Swin t·ª´ th∆∞ m·ª•c repo\ntry: from ram.models.swin_transformer import SwinTransformer\nexcept: import sys; sys.path.append('recognize-anything'); from ram.models.swin_transformer import SwinTransformer\n\n# --- C·∫§U H√åNH ---\nPRETRAINED = 'tag2text_swin_14m.pth'\nJSON_FILE = 'tag2text_train.json'\nTAG_FILE = 'garbage_tags.txt'\nBATCH_SIZE = 4\nLR = 1e-5\nEPOCHS = 1\n\n# --- 1. H√ÄM LOAD CHECKPOINT TH√îNG MINH (B·ªé QUA L·ªñI SIZE) ---\ndef load_smart(model, path):\n    print(f\"üîÑ ƒêang n·∫°p weights t·ª´ {path} (Ch·∫ø ƒë·ªô b·ªè qua l·ªói k√≠ch th∆∞·ªõc)...\")\n    ckpt = torch.load(path, map_location='cpu')\n    state = ckpt.get('model', ckpt)\n    model_dict = model.state_dict()\n    \n    # L·ªçc b·ªè key l·ªách size (V√≠ d·ª•: label_embed c≈© 3429 -> m·ªõi 7)\n    new_state = {}\n    for k, v in state.items():\n        if k in model_dict:\n            if v.shape == model_dict[k].shape:\n                new_state[k] = v\n            # Fix Swin bias (N·ªôi suy k√≠ch th∆∞·ªõc)\n            elif 'relative_position_bias_table' in k:\n                if v.shape != model_dict[k].shape:\n                    new_state[k] = F.interpolate(v.permute(1,0).unsqueeze(0), size=model_dict[k].shape[0], mode='linear', align_corners=False).squeeze(0).permute(1,0)\n    \n    # Load v√†o model (strict=False ƒë·ªÉ kh√¥ng b√°o l·ªói thi·∫øu key)\n    msg = model.load_state_dict(new_state, strict=False)\n    print(\"‚úÖ ƒê√£ n·∫°p xong weights! (C√°c key l·ªách size ƒë√£ b·ªã lo·∫°i b·ªè)\")\n\n# --- 2. CLASS MODEL T·ª∞ ƒê·ªäNH NGHƒ®A (KH√îNG PH·ª§ THU·ªòC FILE .PY C≈®) ---\nclass Tag2Text_Custom(nn.Module):\n    def __init__(self, tag_list_file):\n        super().__init__()\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.tokenizer.add_special_tokens({'bos_token': '[DEC]', 'additional_special_tokens': ['[ENC]']})\n        self.tokenizer.enc_token_id = self.tokenizer.additional_special_tokens_ids[0]\n        \n        # Load Tag List\n        with open(tag_list_file, 'r') as f:\n            self.tag_list = np.array(f.read().splitlines())\n        self.num_class = len(self.tag_list)\n        \n        # Vision: Swin Base (128 dim) - Kh·ªõp v·ªõi checkpoint\n        self.visual_encoder = SwinTransformer(img_size=384, patch_size=4, in_chans=3, embed_dim=128, depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32], window_size=12)\n        \n        # Text: BERT (Config c·ª©ng)\n        # T·ª± t·∫°o config n·∫øu file kh√¥ng t·ªìn t·∫°i ho·∫∑c l·ªói\n        bert_cfg = BertConfig(vocab_size=30524, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, encoder_width=1024)\n        self.text_decoder = BertLMHeadModel(config=bert_cfg)\n        \n        # Heads m·ªõi (7 class)\n        self.tagging_head = nn.Linear(1024, self.num_class)\n        self.label_embed = nn.Embedding(self.num_class, 512)\n\n    def forward(self, img, input_ids=None):\n        emb = self.visual_encoder(img)\n        emb_pool = emb.mean(dim=1)\n        tag_logits = self.tagging_head(emb_pool)\n        \n        loss_text = 0\n        if input_ids is not None:\n            # Truy·ªÅn nh√£n v√†o BERT ƒë·ªÉ sinh caption\n            loss_text = self.text_decoder(input_ids, encoder_hidden_states=emb, labels=input_ids).loss\n        return tag_logits, loss_text\n\n# --- 3. MAIN TRAINING ---\ndef main():\n    # Import Dataset t·∫°i ƒë√¢y ƒë·ªÉ tr√°nh l·ªói import v√≤ng\n    from custom_datasets import Tag2TextDataset\n    \n    device = torch.device('cuda')\n    print(\"üöÄ Training Tag2Text (Custom Version)...\")\n    \n    # Dataset\n    trans = transforms.Compose([transforms.Resize((384, 384)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n    ds = Tag2TextDataset(JSON_FILE, trans)\n    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n    \n    # Model Init & Load\n    model = Tag2Text_Custom(tag_list_file=TAG_FILE)\n    load_smart(model, PRETRAINED) # D√πng h√†m load th√¥ng minh\n    \n    model = model.to(device)\n    optim = torch.optim.AdamW(model.parameters(), lr=LR)\n    crit = nn.BCEWithLogitsLoss()\n    scaler = GradScaler()\n    \n    # Loop\n    for ep in range(EPOCHS):\n        model.train()\n        total_loss = 0\n        for i, (img, tag, txt, _) in enumerate(dl):\n            img, tag, txt = img.to(device), tag.to(device), txt.to(device)\n            optim.zero_grad()\n            with autocast():\n                tag_logits, loss_text = model(img, txt)\n                loss = crit(tag_logits, tag) + loss_text\n            \n            scaler.scale(loss).backward(); scaler.step(optim); scaler.update()\n            total_loss += loss.item()\n            \n            if i%50==0: print(f\"Ep {ep+1} Step {i} Loss: {loss.item():.4f}\")\n        \n        print(f\"=== Epoch {ep+1} Done. Avg Loss: {total_loss/len(dl):.4f} ===\")\n        torch.save(model.state_dict(), \"tag2text_finetuned.pth\")\n    \n    print(\"Done Training Tag2Text!\")\n\nif __name__ == '__main__': main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:05:49.277544Z","iopub.execute_input":"2025-12-03T17:05:49.277881Z","iopub.status.idle":"2025-12-03T17:05:49.287069Z","shell.execute_reply.started":"2025-12-03T17:05:49.277863Z","shell.execute_reply":"2025-12-03T17:05:49.285768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ch·∫°y Training","metadata":{}},{"cell_type":"code","source":"# Ch·∫°y Train RAM\n!python train_ram.py\n\n# Ch·∫°y Train Tag2Text\n!python train_tag2text.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEMO & BENCHMARK (RAM VS TAG2TEXT)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom torchvision import transforms\nfrom ram.models import ram\nimport torch.nn as nn\nimport os\nimport json\nimport random\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nimport matplotlib.pyplot as plt\nfrom transformers import BertTokenizer, BertConfig, BertLMHeadModel\n# Import Swin\ntry: from ram.models.swin_transformer import SwinTransformer\nexcept: import sys; sys.path.append('recognize-anything'); from ram.models.swin_transformer import SwinTransformer\n\n\n# ƒê∆∞·ªùng d·∫´n c·ªßa file model ƒë√£ input\nPATH_RAM = \"/kaggle/input/regconize-anything-model-finetuned/recognize-anything/ram_best.pth\" \nPATH_T2T = \"/kaggle/input/regconize-anything-model-finetuned/recognize-anything/tag2text_finetuned.pth\"\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCLASSES = ['glass', 'paper', 'cardboard', 'plastic', 'metal', 'battery', 'trash']\ntransform = transforms.Compose([transforms.Resize((384, 384)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\n# --- 1. LOAD RAM ---\nprint(\"===ƒêang n·∫°p RAM...===\")\nmodel_ram = ram(pretrained='ram_swin_large_14m.pth', image_size=384, vit='swin_l')\nmodel_ram.tagging_head = nn.Linear(1536, 7)\n\nif os.path.exists(PATH_RAM):\n    print(f\"===ƒê√£ t√¨m th·∫•y: {PATH_RAM}\")\n    model_ram.load_state_dict(torch.load(PATH_RAM, map_location=device))\nelse:\n    print(f\"XXX C·∫¢NH B√ÅO: Kh√¥ng th·∫•y file {PATH_RAM}. ƒêang d√πng model r·ªóng!\")\n\nmodel_ram.to(device).eval()\n\n# --- 2. LOAD TAG2TEXT (Class Inference) ---\nprint(\"‚è≥ ƒêang n·∫°p Tag2Text...\")\nclass Tag2Text_Inference(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.tokenizer.padding_side = 'left'\n        self.visual_encoder = SwinTransformer(img_size=384, patch_size=4, in_chans=3, embed_dim=128, depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32], window_size=12)\n        bert_config_path = 'ram/models/bert_config.json'\n        if not os.path.exists(bert_config_path): bert_config_path = '/kaggle/working/recognize-anything/ram/models/bert_config.json'\n        med_config = BertConfig.from_json_file(bert_config_path)\n        med_config.encoder_width = 1024\n        self.text_decoder = BertLMHeadModel(config=med_config)\n        self.tagging_head = nn.Linear(1024, 7)\n        self.label_embed = nn.Embedding(7, 512)\n\n    def generate(self, img, tags):\n        emb = self.visual_encoder(img)\n        prompt = [f\"A photo of {t}.\" for t in tags]\n        inp = self.tokenizer(prompt, padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\").to(img.device)\n        with torch.no_grad():\n            out = self.text_decoder.generate(input_ids=inp.input_ids, attention_mask=inp.attention_mask, encoder_hidden_states=emb, max_new_tokens=20, num_beams=1)\n        return self.tokenizer.batch_decode(out, skip_special_tokens=True)\n\nmodel_t2t = Tag2Text_Inference()\nif os.path.exists(PATH_T2T):\n    print(f\"===ƒê√£ t√¨m th·∫•y: {PATH_T2T}\")\n    # Load strict=False ƒë·ªÉ an to√†n\n    model_t2t.load_state_dict(torch.load(PATH_T2T, map_location=device), strict=False)\nelse:\n    print(f\"XXX C·∫¢NH B√ÅO: Kh√¥ng th·∫•y file {PATH_T2T}. ƒêang d√πng model r·ªóng!\")\n\nmodel_t2t.to(device).eval()\n\n# --- 3. WIDGET DEMO ---\nprint(\"üöÄ H·ªá th·ªëng s·∫µn s√†ng!\")\nbtn = widgets.Button(description='Demo Nhanh', button_style='primary', layout=widgets.Layout(width='200px'))\nout = widgets.Output()\n\ndef on_click(b):\n    with out:\n        clear_output()\n        if not os.path.exists('garbage_test.json'): print(\"Ch∆∞a c√≥ file test!\"); return\n        with open('garbage_test.json') as f: data = json.load(f)\n        \n        # Mix 4 ·∫£nh\n        items = random.sample(data, 4)\n        mosaic = Image.new('RGB', (384, 384))\n        for idx, pos in enumerate([(0,0), (192,0), (0,192), (192,192)]):\n            mosaic.paste(Image.open(items[idx]['image_path']).convert('RGB').resize((192, 192)), pos)\n        \n        plt.figure(figsize=(6,6)); plt.imshow(mosaic); plt.axis('off'); plt.show()\n        tens = transform(mosaic).unsqueeze(0).to(device)\n        \n        # RAM Predict\n        with torch.no_grad():\n            emb = model_ram.visual_encoder(tens)\n            if len(emb.shape)==3: emb=emb.mean(dim=1)\n            probs = torch.sigmoid(model_ram.tagging_head(emb))[0]\n            # Ng∆∞·ª°ng th·∫•p (5%) ƒë·ªÉ b·∫Øt nhi·ªÅu v·∫≠t th·ªÉ\n            found = [f\"{CLASSES[i].upper()} ({p:.0%})\" for i,p in enumerate(probs) if p>0.05]\n            \n        # T2T Predict\n        with torch.no_grad():\n            emb_t2t = model_t2t.visual_encoder(tens)\n            top_tag = CLASSES[torch.sigmoid(model_t2t.tagging_head(emb_t2t.mean(dim=1)))[0].argmax()]\n            cap = model_t2t.generate(tens, [top_tag])[0]\n\n        print(f\"üì¶ Th·ª±c t·∫ø: {', '.join([i['labels'][0] for i in items])}\")\n        print(\"-\" * 40)\n        print(f\"ü§ñ RAM th·∫•y: {', '.join(found)}\")\n        print(f\"ü¶ú T2T m√¥ t·∫£: \\\"{cap}\\\"\")\n\nbtn.on_click(on_click)\ndisplay(btn, out)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEMO (MOSAIC MIX 4 ·∫¢nh)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom PIL import Image\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nimport matplotlib.pyplot as plt\nimport json\nimport random\nimport os\nimport numpy as np\nfrom transformers import BertTokenizer, BertConfig, BertLMHeadModel\n\n# --- H√ÄM T·ª∞ ƒê·ªòNG T√åM FILE MODEL ---\ndef find_model_path(filename):\n    # 1. T√¨m trong th∆∞ m·ª•c hi·ªán t·∫°i (n·∫øu v·ª´a train xong)\n    if os.path.exists(filename):\n        return filename\n    \n    # 2. T√¨m trong th∆∞ m·ª•c Input (n·∫øu m·ªü l·∫°i session)\n    print(f\"üîç ƒêang t√¨m file '{filename}' trong Input...\")\n    for root, dirs, files in os.walk('/kaggle/input'):\n        if filename in files:\n            full_path = os.path.join(root, filename)\n            print(f\"   ‚úÖ ƒê√£ t√¨m th·∫•y: {full_path}\")\n            return full_path\n            \n    print(f\"   ‚ùå KH√îNG T√åM TH·∫§Y '{filename}'. H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ 'Add Input' output c·ªßa phi√™n tr∆∞·ªõc.\")\n    return None\n\n# --- C·∫§U H√åNH ---\n# T·ª± ƒë·ªông t√¨m ƒë∆∞·ªùng d·∫´n\nRAM_PATH = find_model_path(\"ram_best.pth\")\nT2T_PATH = find_model_path(\"tag2text_finetuned.pth\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCLASSES = ['glass', 'paper', 'cardboard', 'plastic', 'metal', 'battery', 'trash']\ntransform = transforms.Compose([transforms.Resize((384, 384)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\n\n# 1. ƒê·ªäNH NGHƒ®A L·∫†I MODEL\n\ntry: from ram.models.swin_transformer import SwinTransformer\nexcept: import sys; sys.path.append('recognize-anything'); from ram.models.swin_transformer import SwinTransformer\nfrom ram.models import ram\n\nclass Tag2Text_Inference(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.tokenizer.padding_side = 'left'\n        self.visual_encoder = SwinTransformer(img_size=384, patch_size=4, in_chans=3, embed_dim=128, depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32], window_size=12)\n        bert_config_path = 'ram/models/bert_config.json'\n        if not os.path.exists(bert_config_path): bert_config_path = '/kaggle/working/recognize-anything/ram/models/bert_config.json'\n        med_config = BertConfig.from_json_file(bert_config_path)\n        med_config.encoder_width = 1024\n        self.text_decoder = BertLMHeadModel(config=med_config)\n        self.tagging_head = nn.Linear(1024, 7)\n        self.label_embed = nn.Embedding(7, 512)\n\n    def generate(self, img, tags):\n        emb = self.visual_encoder(img)\n        prompt = [f\"A photo of {t}.\" for t in tags]\n        inp = self.tokenizer(prompt, padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\").to(img.device)\n        with torch.no_grad():\n            out = self.text_decoder.generate(input_ids=inp.input_ids, attention_mask=inp.attention_mask, encoder_hidden_states=emb, max_new_tokens=20, num_beams=1)\n        return self.tokenizer.batch_decode(out, skip_special_tokens=True)\n\n\n# 2. LOAD WEIGHTS\n\nprint(\"\\n‚è≥ ƒêang n·∫°p Models...\")\n\n# Load RAM\nmodel_ram = ram(pretrained='ram_swin_large_14m.pth', image_size=384, vit='swin_l')\nmodel_ram.tagging_head = nn.Linear(1536, 7)\n\nif RAM_PATH:\n    model_ram.load_state_dict(torch.load(RAM_PATH, map_location=device))\n    print(\"‚úÖ RAM: ƒê√£ n·∫°p finetuned weights.\")\nelse:\n    print(\"‚ö†Ô∏è RAM: ƒêang d√πng weight g·ªëc (ch∆∞a finetune)!\")\n\nmodel_ram.to(device).eval()\n\n# Load Tag2Text\nmodel_t2t = Tag2Text_Inference()\n\nif T2T_PATH:\n    model_t2t.load_state_dict(torch.load(T2T_PATH, map_location=device), strict=False)\n    print(\"‚úÖ Tag2Text: ƒê√£ n·∫°p finetuned weights.\")\nelse:\n    print(\"‚ö†Ô∏è Tag2Text: ƒêang d√πng weight r·ªóng!\")\n\nmodel_t2t.to(device).eval()\nprint(\"üöÄ H·ªá th·ªëng s·∫µn s√†ng!\")\n\n\n# 3. WIDGET DEMO\n\nbtn = widgets.Button(description='üé≤ Mix & Test', button_style='primary', layout=widgets.Layout(width='200px'))\nout = widgets.Output()\n\ndef on_click(b):\n    with out:\n        clear_output()\n        if not os.path.exists('garbage_test.json'): print(\"‚ö†Ô∏è Ch∆∞a c√≥ file test data! H√£y ch·∫°y Cell 2 tr∆∞·ªõc.\"); return\n        with open('garbage_test.json') as f: data = json.load(f)\n        \n        items = random.sample(data, 4)\n        mosaic = Image.new('RGB', (384, 384))\n        for idx, pos in enumerate([(0,0), (192,0), (0,192), (192,192)]):\n            mosaic.paste(Image.open(items[idx]['image_path']).convert('RGB').resize((192, 192)), pos)\n        \n        plt.figure(figsize=(6,6)); plt.imshow(mosaic); plt.axis('off'); plt.show()\n        tens = transform(mosaic).unsqueeze(0).to(device)\n        \n        # Predict\n        with torch.no_grad():\n            # RAM\n            emb = model_ram.visual_encoder(tens)\n            if len(emb.shape)==3: emb=emb.mean(dim=1)\n            probs = torch.sigmoid(model_ram.tagging_head(emb))[0]\n            found_ram = [f\"{CLASSES[i].upper()} ({p:.0%})\" for i,p in enumerate(probs) if p>0.05]\n            \n            # T2T\n            emb_t2t = model_t2t.visual_encoder(tens)\n            top_tag = CLASSES[torch.sigmoid(model_t2t.tagging_head(emb_t2t.mean(dim=1)))[0].argmax()]\n            cap = model_t2t.generate(tens, [top_tag])[0]\n\n        print(f\"üì¶ Th·ª±c t·∫ø: {', '.join([i['labels'][0] for i in items])}\")\n        print(\"-\" * 40)\n        print(f\"ü§ñ RAM ph√°t hi·ªán: {', '.join(found_ram)}\")\n        print(f\"üìù Tag2Text m√¥ t·∫£: \\\"{cap}\\\"\")\n\nbtn.on_click(on_click)\ndisplay(btn, out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:05:56.861604Z","iopub.execute_input":"2025-12-03T17:05:56.861941Z","iopub.status.idle":"2025-12-03T17:06:39.507557Z","shell.execute_reply.started":"2025-12-03T17:05:56.861924Z","shell.execute_reply":"2025-12-03T17:06:39.506724Z"}},"outputs":[],"execution_count":null}]}